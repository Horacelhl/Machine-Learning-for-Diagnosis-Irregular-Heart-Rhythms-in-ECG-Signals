{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(record_base_path):\n",
    "    files = []\n",
    "    for filename in os.listdir(record_base_path):\n",
    "        if filename.endswith('.mat'):\n",
    "            path = os.path.join(record_base_path, filename)\n",
    "            files.append(path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    min_batch = []    \n",
    "    for i in tqdm(range(len(get_file(record_base_path)))):\n",
    "        data_12 = sio.loadmat(path[i])['ECG'][0][0][2]\n",
    "        jieduan = data_12[:,i:i+3000]\n",
    "        if jieduan.shape[1] <3000:\n",
    "            jieduan = data_12[:,-3000:]\n",
    "        min_batch.append(jieduan[[1,2,6,7,8,9,10,11],:].T) \n",
    "    return np.array(min_batch).reshape(-1,3000,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "    \n",
    "    ## Loss\n",
    "    fig.add_subplot(2,1,1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    fig.add_subplot(2,1,2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=6, activation='relu', input_shape=(input_size)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(3), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=6, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "              \n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "              \n",
    "    #LSTM model\n",
    "    #model.add(layers.LSTM(64, return_sequences=True))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(32, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    #model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(9, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset \n",
    "    filemat = get_file(record_base_path)\n",
    "    X = get_data(filemat)\n",
    "    df = pd.read_csv('./CPSC2018/REFERENCE.csv')\n",
    "    y = pd.get_dummies(df[\"First_label\"][:X.shape[0]])\n",
    "\n",
    "\t# prepare pixel data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
    "\t\n",
    "    # define model\n",
    "    model = define_model()    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    model_checkpoint_callback = ModelCheckpoint(sec_path + 'weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    \n",
    "    # fit model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                      validation_data=(X_val, y_val), callbacks=[model_checkpoint_callback])\n",
    "    elapsed_time = time.time() - start_time # training time\n",
    "    \n",
    "\t# evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # learning curves    \n",
    "    plot_history(history)\n",
    "    loss = float(\"{0:.3f}\".format(loss))\n",
    "    accuracy = float(\"{0:.3f}\".format(accuracy))\n",
    "    elapsed_time = float(\"{0:.3f}\".format(elapsed_time))\n",
    "\n",
    "\n",
    "    #saving model\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    result = [\"Evaluation result: \",\"loss: \"+ str(loss), \"accuracy: \"+ str(accuracy), \"elapsed_time: \"+ str(elapsed_time)]\n",
    "    result = pd.DataFrame(result)\n",
    "    with open(sec_path + 'history.csv', mode='w') as f:\n",
    "        result.to_csv(f)\n",
    "        hist_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6877/6877 [00:22<00:00, 303.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 2995, 64)          3136      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2995, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1498, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1498, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1493, 64)          24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1493, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 747, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 747, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 745, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 745, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 373, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 373, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 371, 32)           6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 371, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 184, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 184, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 92, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 92, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 90, 32)            3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 90, 32)            128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 45, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 45, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 45, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 45, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                46112     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 108,393\n",
      "Trainable params: 107,817\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "175/175 [==============================] - 51s 283ms/step - loss: 1.9192 - accuracy: 0.3035 - val_loss: 3.2623 - val_accuracy: 0.1389\n",
      "Epoch 2/60\n",
      "175/175 [==============================] - 50s 284ms/step - loss: 1.5596 - accuracy: 0.4512 - val_loss: 2.6037 - val_accuracy: 0.2859\n",
      "Epoch 3/60\n",
      "175/175 [==============================] - 61s 348ms/step - loss: 1.4868 - accuracy: 0.4710 - val_loss: 2.1876 - val_accuracy: 0.3845\n",
      "Epoch 4/60\n",
      "175/175 [==============================] - 60s 346ms/step - loss: 1.3367 - accuracy: 0.5505 - val_loss: 2.0422 - val_accuracy: 0.4233\n",
      "Epoch 5/60\n",
      "175/175 [==============================] - 62s 355ms/step - loss: 1.2570 - accuracy: 0.5640 - val_loss: 1.9338 - val_accuracy: 0.4653\n",
      "Epoch 6/60\n",
      "175/175 [==============================] - 61s 349ms/step - loss: 1.1680 - accuracy: 0.6087 - val_loss: 1.7897 - val_accuracy: 0.4782\n",
      "Epoch 7/60\n",
      "172/175 [============================>.] - ETA: 1s - loss: 1.1293 - accuracy: 0.6253"
     ]
    }
   ],
   "source": [
    "# entry point, run the test harness\n",
    "record_base_path = \"./CPSC2018\"\n",
    "sec_path = './results/'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 9\n",
    "input_size = 3000, 8\n",
    "\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sec_path = r'./results/'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "input_size = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=6, activation='relu', input_shape=(input_size)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool1D(pool_size=(3), strides=(2), padding=\"same\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool1D(pool_size=(2), strides=(2), padding=\"same\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#LSTM model\n",
    "#model.add(layers.LSTM(64, return_sequences=True))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(sec_path + 'weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                      validation_data=(X_val, y_val), callbacks=[model_checkpoint_callback])\n",
    "\n",
    "elapsed_time = time.time() - start_time # training time\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test) # evaluating model on test data\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "loss = float(\"{0:.3f}\".format(loss))\n",
    "accuracy = float(\"{0:.3f}\".format(accuracy))\n",
    "elapsed_time = float(\"{0:.3f}\".format(elapsed_time))\n",
    "\n",
    "\n",
    "#saving model\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "result = [\"Evaluation result: \",\"loss: \"+ str(loss), \"accuracy: \"+ str(accuracy), \"elapsed_time: \"+ str(elapsed_time)]\n",
    "result = pd.DataFrame(result)\n",
    "with open(sec_path + 'history.csv', mode='w') as f:\n",
    "    result.to_csv(f)\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder = model.load_weights(sec_path + \"weight.hdf5\")\n",
    "encoded_data = model.predict(X_test)\n",
    "\n",
    "#mpl.rcParams['figure.figsize'] = [12, 8]\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
